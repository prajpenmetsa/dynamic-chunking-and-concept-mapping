[
  {
    "Course Title": "Computer Programming",
    "Course Code": "CS0.101",
    "Course Description": "Welcome to CS0.101 Computer Programming! In this journey, we’ll explore the fundamentals of programming, learning how to communicate with computers to solve real-world problems. You’ll gain essential skills in variables, conditionals, loops, functions, and data structures.\n\nProblem-solving will be a major focus, where we’ll teach you how to break down complex challenges and design efficient algorithms.\n\nThrough hands-on practice and coding assignments, you’ll develop the confidence to write clean, readable, and maintainable code. By the end of the course, you’ll have the skills to embark on exciting programming endeavors and explore various applications.",
    "LOs/COs": [
      "CO1: Demonstrate an understanding of computer programming language concepts like variables, data types, conditional, loop, print statements and functions.",
      "CO2: Ability to use pointers, arrays, initialization, dynamic memory allocation and use gdb to debug issues with the code.",
      "CO3: Design and implement algorithms using recursing, iteration with specific use cases like sorting.",
      "CO4: Design structured data storage using structs, and read/write them to hard disk using file I/O.",
      "CO5: Ability to analyze the complexity of the solution offered in terms of memory and runtime and choose efficient alternatives.",
      "CO6: Develop large scale programs using multiple files, header files and make."
    ],
    "Detailed Syllabus": "Unit 1: Basic computer organization, High level programming languages, assemble code, binary instructions, compilers. Program editing, compilation and execution cycle. Basic data types and their representation, Expressions, Operators and precedence levels, Basic I/O functions, Writing straight-line sequence of code, Conditional Statements (if-then-else, switch case). Functions || Unit 2: Loops (for, while etc.), Arrays, Multidimensional Arrays, Parameter passing mechanisms, Pointers, Strings, Pointer Arithmetic. || Unit 3: Recursion, Program stack, scope and lifetime of variables., heap memory, dynamic memory management, linked lists and memory leaks. || Unit 4: Structures, and Enumerations, Standard libraries for string manipulation, disk file access etc. Preprocessor directives, multi-file programming and Make files."
  },
  {
    "Course Title": "Language Models and Agents",
    "Course Code": "CL3.410",
    "Course Description": "Large Language Models (LLMs) are transforming how tasks are automated and personalized across domains. As LLMs evolve into interactive agents, they are set to drive the next major breakthrough in AI. This course introduces the foundations of LLMs, key capabilities for task automation, and essential infrastructures for building LLM agents. We will explore applications such as generation, tool augmentation, web automation, healthcare, and scientific research, while also addressing current limitations and future directions.\n\nStudents will learn the theoretical foundations of transformer architectures, attention mechanisms, and modern training techniques including pre-training, fine-tuning, and reinforcement learning from human feedback. The course also covers AI agents, multi-agent systems, tool use, reasoning capabilities, and real-world applications. Students will implement key algorithms and build practical systems throughout the course.",
    "LOs/COs": [
      "CO1: Articulate language technologies and transformer concepts. Explain the foundations of natural language processing, word/vector embeddings, self-attention mechanisms, transformer architectures and their variants, as well as probabilistic and causal language modeling.",
      "CO2: Prepare and curate high-quality training data for LMs. Design scalable pipelines for collecting, cleaning, formatting, and balancing datasets across domains and languages to ensure quality and diversity in language model training.",
      "CO3: Train small decoder-only language models from scratch. Understand and implement the end-to-end pipeline for training small-scale autoregressive language models, including tokenizer training, model initialization, and loss optimization.",
      "CO4: Adapt and evaluate language models for specific tasks. Apply pre-training, fine-tuning (including supervised and RLHF alignment), prompting techniques, and evaluation benchmarks to customize LLMs for domain-specific applications and reliably measure their performance (e.g., reasoning, hallucination, memorization).",
      "CO5: Build augmented and agentic LLM systems. Design and implement retrieval-augmented and tool-augmented workflows, and assemble agentic frameworks that leverage LLMs to solve real-world problems through interactive, task-oriented pipelines.",
      "CO6: Address responsible deployment and future extensions. Critically assess ethical, safety, bias-mitigation, privacy, quantization/inference-optimization concerns, and explore multimodal integration and emerging research trends to responsibly deploy and extend language models."
    ],
    "Detailed Syllabus": "Module 1: Background || Course Logistics – Overview of schedule, grading, tools. || Introduction to Language Technologies – Basics of natural language processing tasks. || Word Vectors & Language Models – From word embeddings to classical LMs. || Self-Attention & Transformers – Core mechanism behind modern Language Models. || Transformer Variants – Improvements like BERT, GPT, T5, etc. || Module 2: Building Language Models (Transformers and Pre-training) || Pre-training Language Models – Embedding, Objectives like masked LM, autoregression. || Language Generation – Basics of text generation tasks. || Foundations of LMs – Probabilistic modeling and training setups. || Causal LMs: Architecture & Hyperparameters – GPT-like models, decoding, configs. || Scaling LLMs – Challenges and benefits of scaling parameters and data. || Long Sequence Modeling – Handling long context with efficient attention. || Module 3: Using Language Models || Prompting – Techniques like few-shot, chain-of-thought. || Post-training Data for Fine-tuning – Data curation for model alignment. || Module 4: Enhancing Language Model Capabilities || Supervised Fine-tuning – Task-specific training after pretraining. || Preference Alignment – Reinforcement Learning from Human Feedback (RLHF). || Reasoning, Planning & Understanding (1 & 2) – Emergent capabilities of LLMs. || RAG (1 & 2) – Integrating retrieval for grounding outputs. || Module 5: Orchestrating Language Models: Language Agents || Tool Augmentation – Using tools/APIs within model outputs. || LLM Agents: A Case Study – Building interactive task-oriented agents. || Hallucination & Memorization – Model reliability and factuality issues. || Module 6: Misc and Advanced Topics || LLM Evaluation – Benchmarks and human evaluation techniques. || Safety & Ethics – Fairness, misuse prevention, and responsible AI. || Quantization & Inference Optimization – Speeding up and compressing LLMs. || Vision Transformers – LLMs extended to visual inputs. || Multimodal Pre-training – Training with mixed input types (text, vision). || What Next? – Emerging trends, research frontiers, and challenges."
  },
  {
    "Course Title": "Operating Systems and Networks",
    "Course Code": "CS3.301",
    "Course Description": "This course will provide an overview of the principles and foundations of modern operating systems and networking principles. The course is structured into three parts where each part focusses on different aspects of the operating systems as well as the networking.",
    "LOs/COs": [
      "CO1: Extend the concepts of layering and modularity to build new software systems",
      "CO2: Develop appropriate scheduling/synchronization/memory management/ virtual memory/protection module for a new task-specific operating system.",
      "CO3: Implement an application on the top of given operating system in an efficient manner based on process and thread framework available in the given operating system.",
      "CO4: Architect the given system on the top of operating systems by exploiting the system calls of the given operating system services as far as possible.",
      "CO5: Develop a network-based application by exploiting networking related system calls."
    ],
    "Detailed Syllabus": "Unit 1: Introduction, Process and Memory Virtualization – Scheduling, Memory addressing and Paging, and Networking Overview (10 hours); || Unit 2: Concurrency – Threads and locking mechanisms, Common concurrency problems, Data transmission and Network Technologies (10 hours); || Unit 3: Persistence – File Systems, Protection, Network File Systems and basics of Network Security (6 hours);"
  },
  {
    "Course Title": "Algorithm Analysis and Design",
    "Course Code": "CS 1.301",
    "Course Description": "The objective of the course is to teach techniques for effective problem solving incomputing. The use of different paradigms of problem solving will be used to illustrate clever and efficient ways to solve a given problem. In each case emphasis will be placed on rigorously proving correctness of the algorithm. In addition, the analysis of the algorithm will be used to show the efficiency of the algorithm over the naive techniques.",
    "LOs/COs": [
      "CO1: Articulate language technologies and transformer concepts. Explain the foundations of natural language processing, word/vector embeddings, self-attention mechanisms, transformer architectures and their variants, as well as probabilistic and causal language modeling.",
      "CO2: Prepare and curate high-quality training data for LMs. Design scalable pipelines for collecting, cleaning, formatting, and balancing datasets across domains and languages to ensure quality and diversity in language model training.",
      "CO3: Train small decoder-only language models from scratch. Understand and implement the end-to-end pipeline for training small-scale autoregressive language models, including tokenizer training, model initialization, and loss optimization.",
      "CO4: Adapt and evaluate language models for specific tasks. Apply pre-training, fine-tuning (including supervised and RLHF alignment), prompting techniques, and evaluation benchmarks to customize LLMs for domain-specific applications and reliably measure their performance (e.g., reasoning, hallucination, memorization).",
      "CO5: Build augmented and agentic LLM systems. Design and implement retrieval-augmented and tool-augmented workflows, and assemble agentic frameworks that leverage LLMs to solve real-world problems through interactive, task-oriented pipelines.",
      "CO6: Address responsible deployment and future extensions. Critically assess ethical, safety, bias-mitigation, privacy, quantization/inference-optimization concerns, and explore multimodal integration and emerging research trends to responsibly deploy and extend language models."
    ],
    "Detailed Syllabus": "Module 1: Background || Course Logistics – Overview of schedule, grading, tools. || Introduction to Language Technologies – Basics of natural language processing tasks. || Word Vectors & Language Models – From word embeddings to classical LMs. || Self-Attention & Transformers – Core mechanism behind modern Language Models. || Transformer Variants – Improvements like BERT, GPT, T5, etc. || Module 2: Building Language Models (Transformers and Pre-training) || Pre-training Language Models – Embedding, Objectives like masked LM, autoregression. || Language Generation – Basics of text generation tasks. || Foundations of LMs – Probabilistic modeling and training setups. || Causal LMs: Architecture & Hyperparameters – GPT-like models, decoding, configs. || Scaling LLMs – Challenges and benefits of scaling parameters and data. || Long Sequence Modeling – Handling long context with efficient attention. || Module 3: Using Language Models || Prompting – Techniques like few-shot, chain-of-thought. || Post-training Data for Fine-tuning – Data curation for model alignment. || Module 4: Enhancing Language Model Capabilities || Supervised Fine-tuning – Task-specific training after pretraining. || Preference Alignment – Reinforcement Learning from Human Feedback (RLHF). || Reasoning, Planning & Understanding (1 & 2) – Emergent capabilities of LLMs. || RAG (1 & 2) – Integrating retrieval for grounding outputs. || Module 5: Orchestrating Language Models: Language Agents || Tool Augmentation – Using tools/APIs within model outputs. || LLM Agents: A Case Study – Building interactive task-oriented agents. || Hallucination & Memorization – Model reliability and factuality issues. || Module 6: Misc and Advanced Topics || LLM Evaluation – Benchmarks and human evaluation techniques. || Safety & Ethics – Fairness, misuse prevention, and responsible AI. || Quantization & Inference Optimization – Speeding up and compressing LLMs. || Vision Transformers – LLMs extended to visual inputs. || Multimodal Pre-training – Training with mixed input types (text, vision). || What Next? – Emerging trends, research frontiers, and challenges."
  }
]